# RAG Chatbot

A production-ready Retrieval-Augmented Generation (RAG) chatbot built with FastAPI, Streamlit, LangGraph, and ChromaDB.

## ğŸš€ Features

- **Multi-format Support**: PDF, DOCX, XLSX, CSV, TXT, MD, YouTube videos, Web pages
- **Real-time Processing**: Async document ingestion with LangGraph agents
- **Smart Chunking**: Token-based text splitting with overlap
- **Vector Search**: ChromaDB for similarity search with proper source attribution
- **Chain-of-Thought**: Step-by-step reasoning with explicit reasoning process
- **Web Interface**: Clean Streamlit UI with source deduplication
- **Source Management**: Proper filename extraction and type detection
- **Docker Ready**: Complete containerization

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
- [Development Setup](#ï¸-development-setup)
- [API Endpoints](#-api-endpoints)
- [Configuration](#ï¸-configuration)
- [Architecture](#ï¸-architecture)
- [Supported Sources](#-supported-sources)
- [Workflow](#-workflow)
- [Source Attribution Features](#ï¸-source-attribution-features)
- [Project Structure](#-project-structure)
- [Troubleshooting](#-troubleshooting)
- [License](#-license)

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- OpenAI API key

### 1. Clone and Setup
```bash
git clone https://github.com/your-username/rag-chatbot.git
cd rag-chatbot
cp .env.example .env
# Edit .env with your OpenAI API key
```

### 2. Run with Docker
```bash
docker-compose up --build
```

### 3. Access Applications
- **Frontend**: http://localhost:8501
- **Backend API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs

## ğŸ› ï¸ Development Setup

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run Services
```bash
# Backend
uvicorn backend.api.main:app --reload --host 0.0.0.0 --port 8000

# Frontend (in another terminal)
streamlit run streamlit_app.py --server.port 8501 --server.address 0.0.0.0
```

## ğŸ“š API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/upload_file` | Upload document file |
| POST | `/api/v1/upload_link` | Process web link or YouTube URL |
| DELETE | `/api/v1/delete/{source_id}` | Delete source and embeddings |
| POST | `/api/v1/query` | Query documents with Chain-of-Thought |
| POST | `/api/v1/chat` | Chat interface with conversation context |
| GET | `/api/v1/list_sources` | List uploaded sources with status |
| GET | `/api/v1/status/{source_id}` | Get processing status |
| GET | `/api/v1/health` | Health check endpoint |

## âš™ï¸ Configuration

Environment variables in `.env`:

```text
# Required
OPENAI_API_KEY=your_openai_api_key_here

# Optional (with defaults)
MAX_CHUNK_TOKENS=500
CHUNK_OVERLAP=50
LLM_MODEL=gpt-4o
EMBED_MODEL=text-embedding-3-large
CHROMADB_DIR=./data/vectorstore
API_BASE_URL=http://localhost:8000/api/v1
```

## ğŸ—ï¸ Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit     â”‚â”€â”€â”€â”€â”‚    FastAPI      â”‚â”€â”€â”€â”€â”‚    ChromaDB     â”‚
â”‚   Frontend      â”‚    â”‚    Backend      â”‚    â”‚  Vector Store   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   LangGraph     â”‚
                       â”‚ Ingestion Agentsâ”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components
- **FastAPI Backend**: REST API with async endpoints and proper error handling
- **LangGraph Agents**: Orchestrate document ingestion workflow (ingest â†’ chunk â†’ embed)
- **ChromaDB**: Persistent vector database with metadata storage
- **Streamlit Frontend**: Interactive web interface with real-time status updates
- **RAG Service**: Enhanced retrieval with metadata loading and source attribution
- **Docker**: Multi-service containerized deployment

## ğŸ“„ Supported Sources

### Documents
- **PDF**: Portable Document Format files
- **DOCX**: Microsoft Word documents
- **XLSX**: Microsoft Excel spreadsheets
- **CSV**: Comma-separated values files
- **TXT**: Plain text files
- **MD**: Markdown files

### Web Content
- **YouTube Videos**: Automatic transcript extraction with title and uploader detection
- **Web Pages**: HTML content extraction from any accessible URL

## ğŸ”„ Workflow

### Document Ingestion
1. **Upload** â†’ File/Link received via FastAPI endpoint
2. **Metadata Storage** â†’ Source information saved to JSON files
3. **Extract** â†’ Text extraction using appropriate loader (PDFLoader, YouTubeLoader, etc.)
4. **Clean** â†’ Text normalization and boilerplate removal
5. **Chunk** â†’ Token-based splitting with configurable overlap
6. **Embed** â†’ OpenAI embedding generation with retry logic
7. **Store** â†’ ChromaDB storage with enhanced metadata for source attribution

### Query Processing (Chain-of-Thought RAG)
1. **Embed Query** â†’ Generate query embedding using OpenAI
2. **Retrieve** â†’ Vector similarity search in ChromaDB
3. **Load Metadata** â†’ Fetch source details from metadata JSON files
4. **Aggregate Sources** â†’ Group chunks by source with proper attribution
5. **Construct Context** â†’ Build citations with source information
6. **Chain-of-Thought** â†’ Structured reasoning with explicit steps
7. **Generate Answer** â†’ LLM produces reasoned response with citations
8. **Format Response** â†’ Return answer with sources, reasoning, and proper type detection

## ğŸ·ï¸ Source Attribution Features

- **Filename Extraction**: Proper filename detection for all source types
- **Type Detection**: Automatic classification (Document, YouTube Video, Web Page)
- **Source Deduplication**: Groups multiple chunks from same source
- **Citation Numbers**: Proper [1], [2], [3] citation format
- **Metadata Preservation**: Maintains source information throughout pipeline

## ğŸ“ Project Structure

```text
rag_chatbot/
â”‚
â”œâ”€â”€ backend/                             # FastAPI + LangGraph agents
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py                # /upload_file, /upload_link
â”‚   â”‚   â”‚   â”œâ”€â”€ delete.py                # /delete_file_or_link
â”‚   â”‚   â”‚   â””â”€â”€ query.py                 # /query
â”‚   â”‚   â””â”€â”€ main.py                      # FastAPI app
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                        # Business logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ingestion_service.py         # file/link â†’ cleaned text
â”‚   â”‚   â”œâ”€â”€ chunking_service.py          # splits text into tokenâ€limited chunks
â”‚   â”‚   â”œâ”€â”€ embedding_service.py         # chunk â†’ embed â†’ upsert/delete
â”‚   â”‚   â”œâ”€â”€ deletion_service.py          # remove embeddings by file_id/link_id
â”‚   â”‚   â””â”€â”€ rag_service.py               # retrieve + CoT + answer
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/                          # LangGraph orchestration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ graph.py                     # define nodes & DAG
â”‚   â”‚   â””â”€â”€ nodes/                       # individual async steps
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ ingest_node.py
â”‚   â”‚       â”œâ”€â”€ chunk_node.py
â”‚   â”‚       â”œâ”€â”€ embed_node.py
â”‚   â”‚       â”œâ”€â”€ delete_node.py
â”‚   â”‚       â”œâ”€â”€ retrieve_node.py
â”‚   â”‚       â””â”€â”€ answer_node.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ loader_factory.py            # detect file type & route 
â”‚       â”œâ”€â”€ link_parser.py               # HTML scraper / YouTube â†’ transcript
â”‚       â””â”€â”€ text_cleaner.py              # whitespace, boilerplate removal
â”‚
â”œâ”€â”€ frontend/                            # Streamlit UI
â”‚   â””â”€â”€ streamlit_app.py                # upload widget, link input, query box
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.yaml                      # chunk_size, overlap, top_k, model names
â”‚   â””â”€â”€ logging.yaml                     # uvicorn + agent logging
â”‚
â”œâ”€â”€ .env                                 # secrets & paths
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/                         # raw user files
â”‚   â”œâ”€â”€ transcripts/                     # raw YouTube text
â”‚   â”œâ”€â”€ chunks/                          # cached chunked .json per source
â”‚   â”œâ”€â”€ metadata/                        # mapping file_idâ†”hashâ†”status
â”‚   â””â”€â”€ vectorstore/                     # ChromaDB storage
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.base                  # Python deps & system libs
â”‚   â”œâ”€â”€ Dockerfile                       # builds backend + frontend container
â”‚   â””â”€â”€ requirements.txt                 # pinned Python packages
â”‚
â”œâ”€â”€ docker-compose.yml                   # spins up backend + Streamlit services
â””â”€â”€ README.md
```

## ğŸ“Š Monitoring

- **Health Checks**: `/health` endpoint for service monitoring
- **Structured Logging**: Comprehensive logging with source tracking
- **Processing Status**: Real-time status updates (processing, completed, failed)
- **Error Handling**: Graceful error handling with retry mechanisms
- **Auto-refresh**: Frontend auto-updates processing status

## ğŸ”§ Development

### Adding New File Types
1. Create loader class in `backend/utils/loader_factory.py`
2. Register in `LoaderFactory.get_loader()` method
3. Update file type validation in upload endpoints
4. Test with sample files

### Extending RAG Functionality
1. **New Nodes**: Create in `backend/agents/nodes/`
2. **Workflow Updates**: Modify `backend/agents/graph.py`
3. **Service Layer**: Update relevant service classes
4. **API Endpoints**: Add new routes as needed

## ğŸš¨ Troubleshooting

### Common Issues

#### Source Attribution Problems
- Check metadata files in `data/metadata/`
- Verify source_id consistency across pipeline
- Review embedding storage metadata

#### YouTube Processing Errors
- Ensure yt-dlp is installed and updated
- Check video accessibility and transcript availability
- Verify URL format (youtube.com, youtu.be supported)

#### ChromaDB Dimension Mismatch
- Check embedding model consistency
- Clear vector store if changing models
- Verify OpenAI API key has access to chosen model

#### Memory Issues
- Reduce `MAX_CHUNK_TOKENS` for large documents
- Increase `CHUNK_OVERLAP` for better context
- Monitor container memory limits

### Debug Information

Enable debug logging by checking application logs:

```bash
# Docker
docker-compose logs -f rag-backend
docker-compose logs -f rag-frontend

# Local development
# Check console output for detailed source attribution logs
```

### Data Persistence
- **ChromaDB**: Stored in `data/vectorstore/`
- **Metadata**: JSON files in `data/metadata/`
- **Uploads**: Original files in `data/uploads/`

To reset all data:
```bash
rm -rf data/
```

## âš¡ Performance Optimization

- **Async Processing**: Non-blocking document ingestion
- **Connection Pooling**: Optimized OpenAI API usage
- **Caching**: Metadata file caching for faster retrieval
- **Chunking Strategy**: Balanced chunk size for optimal retrieval

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ™ Acknowledgments

- [OpenAI](https://openai.com/) for the embedding and language models
- [ChromaDB](https://www.trychroma.com/) for the vector database
- [LangGraph](https://github.com/langchain-ai/langgraph) for workflow orchestration
- [Streamlit](https://streamlit.io/) for the web interface
